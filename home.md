项目背景

目前在网络上媒体内容已经无处不在，特别是在社交网络中，大量用户上传与阅览大规模的照片、视频以及文档文件。这些媒体内容，也叫二进制对象（blobs），具有一次上传、被用户频繁访问、不会被更改以及极少删除的特点。

设计存储这些媒体内容的系统面临许多挑战：媒体内容有范围从KBytes到GBytes的不同大小分布，系统需要有效存储大媒体以及小媒体对象；在互联网世界, 每天有极快的对象存储访问请求的增长，系统需有具有低开销地进行线性扩展的能力；面对具有读、写与删除等不同请求类型以及请求不同大小与年龄分布的blob的工作负载，加上可能的系统扩展，都会造成系统负载不均衡，导致系统延迟高吞吐率低；用户上传过程要快速、持久化以及高可用性，并且要求即使在一部分系统故障的情况下全球用户都能在低延迟下快速访问到对象，这需要数据能够被有效复制到全球的数据中心并保证低延时访问。

GFS、Hadoop以及Ceph等能够处理大规模的大文件，但是这些系统维护名字空间以及大量元数据，对于对象存储造成不必要的开销；对于GemFire、Dynamo以及Cassandra等键值对存储系统，虽然能够处理大量小对象，但因为提供强一致性保证而在节点增加或减少时造成大量数据移动，这对于不可变对象不是必需的而引入了其他开销；Haystack以及Twitter Blob Store也处理大规模不可变对象，但是没有解决系统扩增时的负载均衡问题。

设计目标

ZeroFS被设计成完全去中心化的多租户的地理分布式的系统，可存储全球的大规模数据大小范围从KBytes到GBytes的不可变的媒体内容对象，并且提供高吞吐量与低延迟的保证。有以下设计目标：

1）低延迟与高吞吐量：系统被设计来在廉价的设备（HDD）上短时间服务大量请求。ZeroFS使用Exploiting the OS cache，当硬盘与网络缓冲数据交换时zero-copy，(这也是ZeroFS 名称的由来)。从多个节点存储与检索数据的chunking分块，配置读写对象的复制系数以及无性能损失的故障检验技术等实现。

2）地理分布式操作：为了高可用性与持久性，对象(Object)需要在即使出现故障的情况下复制到全球的分布式数据中心中。ZeroFS被设计为去中心化系统，支持跨数据中心双活，数据能够被读取或者写入到任意复制块中。其会将要写的数据写入到最近的数据中心并异步复制到其他数据中心，读取的数据不在当前数据中心时会将请求转发到其他数据中心读取。

3）可扩展性：为了应对处理持续增长的数据需求，系统需有具有低开销地进行线性扩展的能力。为了达到目的才有以下策略：ZeroFS将对象的物理分布与其逻辑分布分开，这样物理分布的变更对于逻辑分布是透明的；ZeroFS设计为完全去中心化的系统，没有主节点；ZeroFS使用index segment来检索对象。

4）负载均衡：系统扩增时要进行负载均衡。ZeroFS在静态稳定的集群中对随机选择的数据节点使用chunking传输大对象来维持负载均衡。当系统动态扩增时，使用再平衡机制将系统恢复为平衡态。

#架构设计

总体上ZeroFS由3部分组件组成，分别为存储与检索对象的一组数据节点（DataNodes），路由请求的前端节点（FrontendNodes）以及协调与维护集群的集群管理器（ClusterManager）。前端节点与数据节点相互完全独立，集群管理器则使用Zookeeper进行同步。


##分区

ZeroFS随机地将blob对象组成虚拟的单元组Partition。相比于Chord与CRUSH算法直接将对象映射到物理机器上，对象到partition的映射以及partition到物理机器的映射在不同阶段进行。这种将逻辑分布与物理分布解耦的方式有利于再平衡rebalancing时数据透明迁移，并且避免集群扩增时立即rehash的开销。


Partition是通过仅允许向后添加的预先分配的大日志文件实现。并且partition是固定大小的，需要足够大又要考虑并行重建耗时少。对象被顺序写入到partition中作为put entry或者delete entry，两种对象条目包含存储对象在partition中偏移的对象头以及对象id。对象id为对象唯一标志，由8字节的partitionId以及32字节的UUID组成，通过前端节点在put操作时产生，且相同对象的多次put操作产生的id相同，在get或者delete操作时用来定位对象。对于put操作，为了数据平衡的目的，partition是随机选择的。对于get与delete操作，partitionId由对象id中提取出来。Put entry除了包含对象内容还包含对象的属性信息，比如对象的大小、对象存活时间（TTL）、创建时间、类型以及用户自定义元数据等等。由于对象不可修改，通过在partition中添加包含删除标志的delete entry用来实现软删除。

为了实现高可用性以及容错处理，partition被作为复制单位被复制到多个不同节点上。在一个数据中心中不能有多于一个的副本以及副本需要在多个数据中心的条件限制下，通过基于硬盘剩余空间的贪婪算法来决定副本分布。Partition有多个状态，在刚创建后是可读可写的read-write状态，服务于所有操作。当大小达到阈值时变为只读状态read-only，仅服务于get与delete操作，注意阈值要小于最大上限值，因为delete也会在partition中增添条目。删除的对象可以在周期性的压缩机制下被从partition上清理掉，如果此后只读的partition拥有足够剩余空间会再变为read-write状态。

操作

ZeroFS实现轻量级的API支持put，get与delete这3种操作。前端节点接受到请求时首先检测请求的安全性，然后利用拥有请求处理逻辑的路由库Router Library来选择partition，然后与数据节点交互来服务。


操作被处理在拥有任一副本的多个数据节点上进行。副本的数目由用户策略决定，这种策略同Cassandra的一致性层级类似。对于put操作来说，出于对持久性与延迟度的权衡，请求被转发到所有拥有副本的节点，然后根据策略定义的回应次数决定k是否操作成功。对于get操作，处于对资源使用率与延迟度的权衡，策略定义随机选择要操作的副本数目k，实践中均使用k=2的副本数目来达到权衡。

Put操作时，前端节点收到请求，然后选择一个数据节点交互并将数据同步地写入到数据节点上。此阶段完成后可视为请求操作成功，之后对象会通过轻量级的异步算法被复制到其他多个地区的数据中心，来保证低延迟与高吞吐量。Delete操作与Put操作类似，只是增加delete entry。


Get操作时，当前端节点不能在当前数据中心检索出对象时，其会将请求转发到其他数据中心并在其他数据中心上返回结果。尽管代理转发的代价很高，但是发生的概率小于0.001%。


负载均衡

多变化的工作负载，大量的大对象以及集群扩增都会造成负载不均衡，影响系统的延迟性与吞吐量。因此ZeroFS在静态与动态集群上实现基于硬盘空间与请求率的负载均衡。

静态集群：使用chunking技术将大对象分成小块chunk，在put操作时将这些chunk随机路由到partition，做到每个partition的大小均衡；使用CDN处理热数据降低partition成为热点的可能性。

动态集群：由于read-write状态的partition接受所有的写请求与大多数的读请求，因此read-write状态的partition是负载不均衡的主要原因之一。特别当系统扩增时，新的数据节点只包含read-write状态的partition，而旧的数据节点大多数只含read-only状态的partition。为了解决这个问题，ZeroFS部署了再平衡策略。

再平衡策略：ZeroFS定义负载均衡的理想状态为三元组（idealRW，idealRO，idealUsed）来分别表示理想的read-write状态的partition，理想的read-only状态的partition以及每个硬盘理想的磁盘使用率。策略通过两阶段策略在磁盘上移动partition使其达到理想状态：阶段1将所有超出理想状态的partition移动到partition pool中；阶段2通过round-robin算法将partition pool中的partition移动到低于理想状态的硬盘上，其可能造成新副本的生成与旧副本的删除。

集群管理器（Cluster Manager）

集群管理器维护Cluster Map，控制拓扑结构，维护状态并且协调集群操作。每个数据中心都有其本地的Cluster Manager并通过Zookeeper与其他保持同步。状态数据由2部分组成：

1）硬件布局Hardware Layout：ZeroFS工作在异构的硬件与配置环境，因此硬件布局包含了机器列表、每台机器上的硬盘以及每个硬盘的容量。布局还维护了资源（机器与磁盘）的状态（健康UP或故障DOWN）并指定主机名与端口，通过其访问数据节点。


分区布局Partition Layout：包含了partition副本信息、partition状态（read-only或read-write）。Cluster Manager周期性地与数据中心交互更新partition状态信息。一个partition的副本可以存在一个数据中心的多个数据节点上或者不同数据中心中。数据节点以及前端节点都可以访问Cluster Map，并始终使用当前状态做出决策，决策涉及到选择可用的机器、过滤副本以及定位对象的位置，分区布局更新才表示一个partition加入成功。


前端节点（Frontend）

前端节点是处理ZeroFS处理外部请求的外部节点，每个数据中心都有其前端节点。由于Cluster Manager来存储状态信息，因此前端节点是去中心化的，执行相同任务且无状态，提高了可扩展性。请求可以转达到其他节点，提高了容错性。且故障的前端节点可以被很快替代，因而提高了故障恢复率。其有以下主要责任：

1）处理请求Request Handling：使用Router Library接收请求并路由到其他数据节点并返回回应。

2）安全性检测Security Checks：执行安全性检测检测，如是否有病毒以及请求授权。

3）捕获操作Capturing Operations：将事件推到ZeroFS外的捕获系统，如使用Kafka进行离线分析。

路由库（Router Library）

路由库具有处理请求的核心功能并与数据节点交互功能。前端节点与客户端都可以使用路由库来服务请求。有以下4个过程：

1）基于策略的路由Policy Based Routing：路由库决定请求处理的partition，对于put操作随机选择partition，对于get与delete操作从对象id中提取partitionId。策略决定要交互的数据节点中的副本数目（{one，k，majority，all}），然而与副本交互。

2）分块技术Chunking：由于大对象会影响负载均衡，因此将大对象切为多个固定大小的小对象，称为chunk。由于chunk大小太大则无效果，太小又回加重工作负载，因此chunk大小定为4～8MBytes。


在put操作时，一个对象b会被顺序切分为k个chunk，每个chunk都视为一个独立的一般对象，其ChunkId同样由8字节的partition的id以及32字节的UUID组成，均经过相同的put操作步骤加入到partition中，很大程度上会被放置在不同数据节点的partition上，因而可以并行写入。为了方便检索原始对象b，ZeroFS创建b的元数据对象bmetadata，其包含对象切分出的chunk数目以及顺序的ChunkId。元数据bmetadata对象也被视为一般对象，在所有put操作完成后ZeroFS返回用户bmetadata的id来作为b的id。如果写入chunk操作过程中途失败，则会删除已经被写入的chunk并重新执行操作。

在get操作时，对象的元数据对象bmetadata被检索出来，并从其中提取所有chunk的id。然后ZeroFS使用大小为s的滑动窗口缓冲区来获取对象数据，这是由于每个chunk可能分布于不同数据节点的partition上，因而可以并行地获取chunk数据。当滑动窗口缓冲区中最开始的chunk已经被获取到，则缓冲区滑动一个chunk位置，并开始向用户返回对象数据。

3）零损失的故障检测Zero-cost Failure Detection：大型系统中发生故障频率高，比如进程无响应、网络连接超时以及硬盘IO问题等，因此ZeroFS需要通过故障检测机制来发现无法服务的数据节点或者硬盘，并避免将请求转发给他们。


ZeroFS使用心跳heartbeats与ping而无须其他额外消息来实现零损失的故障检测，十分简单有效，消耗极少带宽。ZeroFS在check_periodsh时间段内跟踪数据节点或者硬盘连续处理请求失败的次数，如果次数超过阈值，则数据节点在wait_period内被标记为temporarily_down状态，此时发送能够到此数据节点的请求会最终超时并需要用户自己重新尝试。当wait_period过去后，数据节点状态更新为temporarily_available，此时如果下一个请求又失败则又标记为temporarily_down状态，否则可视为可以正常工作状态。

4）转发请求Proxy Requests：ZeroFS通过转发请求到其他数据中心的副本来处理实现高可用性。

数据节点（Datanode）

数据节点维护实际的对象数据，管理硬盘并且对于partition上的副本请求进行回复。为了减少读与写的延迟，使用以下技术：

1）Indexing blobs索引对象：数据节点在内存上维护每个partition副本上对象偏移的索引。索引项由对象id排序，索引每项包含对象id到对象开始偏移的映射、删除标志以及TTL（time to live）。当put操作或者delete操作时均会更新index。


与SSTable类似，ZeroFS限制了内存中index大小，将其切分成多个index segment，将旧的索引片段保存到硬盘上，并为每一个硬盘上的索引片段维护一个内存上的Bloom filter，用来快速过滤需要加载的硬盘索引片段降低硬盘检索延迟。索引片段按照逆时间顺序排序，这样查找一个对象时，如果对象被删除，其能够保证delete entry会比其put entry先找到来防止获取删除对象的数据。当数据中心故障，整个索引还可以根据partition重建。

2）Exploiting OS cache充分利用缓存：由于许多最近写入的数据都会被缓存，越多读请求在内存中获取数据可以极大提高性能，因此充分利用内存空间，限制数据节点中其他数据结构使用的内存空间。

3）Batched writes，with a single disk seek批处理写入：对于同一个partition的写入操作进行批处理然后周期性地刷新写入硬盘，这样只需要一次硬盘寻道后顺序写入。对于刷新周期可以考虑性能与持久性的权衡。

4）Keeping all file handles open保持所有文件句柄打开：由于每个partition大小都非常大（通常100GBytes），因此一个数据中心上的partition数目很少，可以保持所有文件句柄打开。

5）Zero copy gets零拷贝获取：由于读取一个对象不对数据进行计算，因此使用零拷贝策略让内核直接从硬盘拷贝数据到网络缓冲区而不经过应用程序。

复制

为了保证副本一致性，ZeroFS使用异步复制算法来周期性地同步副本，算法是完全分布式的，每个副本都视为主节点并与其他节点同步。同步策略根据pull-based方法从其他副本独立地获取失去的副本的对象，使用异步的两阶段复制协议：

1）阶段1：根据同步点从其他副本获取从同步点后写入的对象的id列表，并过滤出本地副本缺失的对象。

2）阶段2：请求所有缺失的对象数据，并向后添加到副本中。


为了可以快速获悉最近写入的对象，复制算法维护一个称为journal的日志数据结构。Journal是分区最近对象在内存的缓存，通过偏移排序。同步点为最近的lastestOffset，阶段1通过两副本的journal从lastestOffset比对缺失的对象列表，阶段2传输缺失的对象数据然后添加到副本后，最后更新journal，indexing以及lastestOffset。为了提高性能与可扩展性，复制算法使用线程池来在不同时间段运行不同数据中心间或者数据中心内部的复制算法，以及对于相同partition的副本对象请求进行batch批处理来优化。